\section{Background and Preliminaries} \label{sec:background}
%%This section reviews several key concepts for our proposed computational machine self-confidence framework. To make the concepts discussed throughout the paper concrete and provide an accessible proof-of-concept testbed in later sections, we also describe a motivating APS application example inspired by ongoing research in unmanned robotic systems.  
%%\nisar{for me todo: refactor this section: start with overview of what will be covered, then talk about MDPs more formally and give details for reward, etc.; map Donut Delivery application to other robotic autonomous system applications -- note that Donut Delivery will be revisited in Experiments with users later as a basic test (can maybe include user's go/no-go call in Figure 1 somehow?)... VERY IMPORTANT: NEED TO ADD RELATED/PRIOR WORK SECTION!! need to discuss limitations and motivate why \famsec{} is needed !! Then section 3 should be \famsec{}, with first big subsection on \xO{} and second big subsection on \xQ{} (with numerical examples)...section 4 experiments, with details (e.g. of how sim generated/typical reward dists and road nets) and extended results...section 5 discussion, section 6 conclusions...}

\subsection{Related Work}
%%\nisar{VERY IMPORTANT: add subsection to expand on related work: esp. Dragan's critical states and Srinivasa's trust POMDPs, other versions of self-confidence -- need to discuss limitations and motivate why \famsec{} is needed !! Might also be good to justify why a meta-analysis framework is appropriate...}
The problem of identifying and designing algorithmic assurances for mediating trust in human-autonomous robot interaction has many deep connections to the growing literature on explainable/interpretable AI and machine learning \nisar{(refs)}, as well as established and ongoing human factors research on trust in automated systems \nisar{(refs)}. 
Ref. \cite{Israelsen2018-qz} provides a comprehensive survey on these connections. 
In order to motivate the concept of machine self-confidence, we highlight a few recent developments from the algorithmic assurance and autonomous robotics literature that are most closely related to the present work. 

Building on the trust cycle and analysis framework of McKnight, et al. \nisar{[cite]}, \cite{Israelsen2018-qz} notes that all practical algorithmic assurances seek to calibrate one or more of the key `trustworthiness factors' that govern human-autonomy interaction --- namely, the autonomous system's the reliability, competency, and predictability. As such, algorithmic assurance strategies can be generally classified along a wide continuum of related techniques. These range from techniques that implicitly design or reprogram autonomy in desirable/trustworthy ways --- and are thus integral to system behavior and performance (e.g. see \nisar{refs.[...]}) --- to supplemental assurance techniques that provide `post hoc' behavior artifacts for users to assess the system's trustworthiness, without changing its underlying programming (e.g. see \nisar{refs.[...]}). Whereas each end of the spectrum places different burdens on autonomous system designers and users for ensuring appropriate calibration of trust, there is growing evidence to suggest that algorithmic assurances which incorporate key aspects of both integral and supplemental techniques can 

...trust modeling/monitoring for automatic control: Yue Wang from Clemson and other similar kinds of things -- limitations: trust is not just univariate: their approach works well for certain kinds of shared teleop tasks where `trust' can be ascribed to physical gestures in limited context -- but hard to scale up to much more complex problems that rely on higher level reasoning and capabilities for autonomy, e.g. perception, planning, reasoning, communication, etc. 

...

\subsection{MDP-based Planning} \label{sec:mdp}
%%The diversity of factors that influence APS self-confidence requires a rich modeling approach for algorithmic assessment. 
We establish algorithmic realizations of self-confidence assessments by initially studying APS capabilities that can be modeled as Markov decision processes (MDPs). MDPs are composed of finite states and actions for the APS that partially resolve the nondeterminism in state transitions by deciding from what probability distribution $p(\cdot)$ the next state will be sampled. %%The co-existence of nondeterministic and stochastic choices in MDPs are expressive enough to account for a range of uncertainties including adversarial environmental factors and inaccuracies in execution. %%, and limitations in prior knowledge (e.g. imperfect knowledge of $p(\cdot)$).  
MDPs have well-established connections to other widely used approaches for autonomous decision-making and learning under uncertainty, such as partially observable MDPs (POMDPs) for decision-making in limited observability environments and reinforcement learning for decision-making with incomplete model information \cite{Kochenderfer2015-uu}. As such, they provide an ideal starting point for an initial analysis of self-confidence that can be generalized in future work. 

More formally, we consider generic MDP formulations of a task \task{} delegated to an APS. In an MDP framing of \task{}, the autonomous agent must find an optimal policy $\pi = u(x)$ for an MDP with dynamical state $x$ and actions $u$, such that the objective function
$U = \mathbb{E} \left[\sum_{k=0}^{\infty} \gamma^i r(x_k,u_k) \right]$ is maximized for all times $k=0,...,\infty$ --  
where $R(x_k,u_k)$ rewards (penalizes) the APS for being in (un)desirable states and taking (un)desirable actions, $\mathbb{E}[\cdot]$ is the expected value over all possible future states, and $\gamma \in (0,1]$ is a (tunable) future discount factor. 
Given any $u_k$, the state $x_k$ updates via a Markov probabilistic transition model $x_{k+1} \sim p(x_{k+1}|x_{k},u_{k})$,  
i.e. $x_{i}$ is fully observed at time $i$ (no sensor noise), while transitions $i\rightarrow k+1$ have random perturbations.
In a fully posed MDP, $\pi$ is the optimal state-action policy, which can be recovered from Bellman's equation via dynamic programming. 
However, in many practical cases, policy approximations $\tilde{\pi}$ are required to cope with complex or uncertain dynamics (e.g. for reinforcement learning or very large state spaces) \cite{Kochenderfer2015-uu}. 
    
    
	\begin{figure}[t]%[htbp]
    	\centering
     	\includegraphics[width=0.5\textwidth]{Figures/RoadNet}
    	\caption{ADT in road network evading MG with information from noisy UGS. \brett{perhaps put a user in the loop to highlight that this is a human-robot system}} 
        \label{fig:RoadNet}
        \vspace{-0.2cm}
    \end{figure}    
    
\subsubsection{Donut Delivery Application} \label{sec:donut_delivery}
Consider a concrete grounding example called the `Donut Delivery' problem (based on a `VIP escort' scenario~\cite{Humphrey2012-lr}). As shown in Fig.~\ref{fig:RoadNet} , an autonomous donut delivery truck (ADT) navigates a road network in order to reach a delivery destination while avoiding a motorcycle gang (MG) that will steal the donuts if they cross paths with the delivery truck. The motorcycle gang's location is unknown but can be estimated using intermittent updates from unmanned ground sensors (UGS). The delivery truck's decision space involves selecting a sequence of discrete actions (i.e. go straight, turn left, turn right, go back, stay in place). The ADT motion, UGS readings, and MG behavior are stochastic, and the problems of decision making and sensing are strongly coupled: some trajectories through the network might allow the ADT to localize the MG before heading to the delivery destination but incur a high time penalty); other trajectories may afford rapid delivery with high MG location uncertainty but increase the risk of getting caught by the MG, which can take multiple paths. A human supervisor monitors the ADT during operation. The supervisor does not have detailed knowledge or control of the ADT---but can interrogate its actions, and (based on whatever limited amount of information is provided to the supervisor) can provide `go' or `no go' commands to proceed with or abort the delivery operation before it commences. 
%, as well as potentially modify its decision making stance (`aggressively pursue exit' vs. `be very conservative and cautious') in order to better cope with the MG (which is sporadically observed and follows an unknown course). \nisar{modify last sentence -- just state go/no go?}

The physical states describing the combined motion of the ADT (whose states are always perfectly observable) and MG can be discretized in time and space to produce a Markov process model defined by some initial joint state probability distribution and joint state transition matrix, which depends on the steering actions taken by the ADT. 
%The probability of obtaining `detection' and `no detection' data from each UGS given the true state of the MG can be modeled and used to update probabilistic beliefs about the MG's state. 
%Finally, 
A reward function $R(x_k,u_k) = R_k$ can be specified for each time step $k$ to encode user preferences over the combined state of the ADT and MG, e.g. $R_k = -200$ for each time step the ADT is not co-located with the MG but not yet at the goal, $R_k= -2000$ if the ADT and MG are co-located, and $R_k=+2000$ if the ADT reaches the goal without getting caught. 
%Given these elements, the ADT's navigational planning and decision-making problem may be generally formulated as a POMDP. 
%If the MG's state is fully observable at each step $k$ (e.g. due to very reliable and accurate UGS that cover all areas of the road network), the problem reduces to an MDP. 
If the MG's state is observable at each step $k$ (e.g. due to very reliable and accurate UGS that cover all areas of the road network), the ADT's planning problem can be framed as an MDP. 

