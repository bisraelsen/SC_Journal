\section{Introduction}
% Main goal: Argue for SC, and \xQ. Present \xQ{} and justify decisions. Demonstrate the performance on `realistic simulations'.
Autonomy defines the ability of a physical system to perform a complex tasks without human intervention for extended periods of time. This requires at least one or more of the capabilities of an artificially intelligent physical agent: reasoning, knowledge representation, planning, learning, perception, motion/manipulation, and communication \cite{Israelsen2017-ym}. 
Yet, an autonomous physical system (APS) always interacts with a human user in some way \cite{Bradshaw2013-ck}.  That is, the aforementioned capabilities are only the means by which an APS achieves some \emph{intended} degree of self-sufficiency and self-directedness for tasks \emph{delegated} by a user to meet an `intent frame' (desired set of goals, plans, constraints, stipulations, and/or value statements) \cite{Miller2014-av}. 
As capabilities have continued to improve and find new applications, APS are also becoming more complex, opaque and difficult for users (as well as designers and other stakeholders) to fully comprehend. For APS characterized by state of the art uncertainty-based AI and data-driven machine learning, it can become extremely difficult to reconcile predictions about behavior and performance limits with desired intent frames in noisy, untested, and `out of scope' task conditions. This ushers in questions related to user trust in autonomous systems, i.e. a user's willingness in depending on an APS to carry out a delegated set of tasks, in light of the user's own understanding of the APS capabilities. 

This work examines how APS can be designed to actively adjust users' expectations and understanding of APS capabilities which inform trust. As surveyed in \cite{Israelsen2017-ym}, several broad classes of \emph{algorithmic assurances} for APS have been developed for this purpose. 
Assurances are challenging to develop because they must allow users to gain better insight and understanding of APS behaviors for effectively managing operations, without undermining autonomous operations or burdening users. 
Process-based assessment and meta-analysis techniques allow APS to self-qualify their capabilities and competency boundaries, by evaluating and reporting their associated degree of `self-trust' or \emph{self-confidence} for a particular task. 
%This resonates with the concept put forth by \cite{Hutchins2015-if}, which proposed using human expert evaluations of specific APS capabilities to manually encode where and when these may break down in particular tasking situations. However, to be useful in real applications, APS must be able to form these evaluations on their own. 
Several definitions and algorithmic approaches have been proposed recently which enable APS to automatically generate self-confidence scores in the context of different tasks and uncertainty-based capability assessments \cite{Sweet2016-tz, Israelsen2017-ym}. This work restricts attention to the following definition of self-confidence, which captures the others to a large extent: \textit{an agent's perceived ability to achieve assigned goals (within a defined region of autonomous behavior) after accounting for (1) uncertainties in its knowledge of the world, (2) uncertainties of its own state, and (3) uncertainties about its reasoning process and execution abilities.} However, a framework for computing and communicating self-confidence for general decision-making autonomy architectures has yet to be established. Furthermore, it has not yet been confirmed experimentally whether (or in what contexts) human-APS interfaces that incorporate self-confidence reporting improve the ability of users to delegate of tasks within APS competency limits, compared to status quo interfaces that do not use such reporting. This paper provides results aimed at addressing both of these gaps. 

%\nisar{can probably trim this parag down a bit, after editing per comment at end...}

In Section 2, we motivate the machine self-confidence idea and ground the concept for autonomous planning problems in the context of general Markov Decision Processes (MDPs), using a concrete uncertain navigation application example where tasks are delegated by a user. We review a factorization based framework for self-confidence assessment (\famsec), and in Section 3 present a novel strategy for computing the so-called `solver quality' factor of self-confidence, drawing inspiration from empirical hardness modeling literature \cite{Leyton-Brown2009-yr}. Numerical examples for the UGV navigation problem under different MDP solver, parameter, and environment conditions, indicate that the self-confidence scores exhibit desired properties. Section 4 describes the setup and initial outcomes of experimental trials to investigate the effects of reporting self-confidence to users on simulated instances of the UGV navigation problem. The results show significantly improved delegated task performance outcomes in conditions where self-confidence feedback is provided to users vs. conditions where no self-confidence feedback is provided, providing favorable evidence for the efficacy of self-confidence reporting. Section 5 presents conclusions and avenues for future investigation. %\nisar{only mention solver quality -- what about outcome assessment? this is also in the data...}
