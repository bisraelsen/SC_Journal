\section{Introduction}
% Main goal: Argue for SC, and \xQ. Present \xQ{} and justify decisions. Demonstrate the performance on `realistic simulations'.

Thanks to advances in AI and machine learning, unmanned autonomous physical systems (APS) are poised to tackle complex decision making problems for high-consequence applications, such as wilderness search and rescue, transportation, agriculture, remote science, and space exploration. 
%Unlike low-level automation for assembly lines, cruise control, thermostats, etc., 
APS must be self-sufficient and make self-guided decisions about complex problems delegated by users. Hence, APS that are taskable---able to translate high-level commands into suitable processes for sensing, learning, reasoning, communicating, and acting --must also be cognizant and knowledge-rich--capable of reasoning about the capabilities and limitations of their own processes, anticipating possible failures, and able to recognize when they are operating incorrectly \cite{david2016defense}. %to adapt accordingly. %To ensure long-term robustness and resilience for minimally supervised operations, APS behaviors must be predictable, understandable, and explainable. %to human users/stakeholders. %, who in many cases can also provide collaborative high-level assistance or supervisory directives in difficult situations. 

This work is motivated by the need to develop new computational strategies for assessing when an APS reaches its \emph{competency boundaries}. If computed and communicated correctly, such assessments can provide users with clearer predictions of APS behavior and understanding of actual APS capabilities. This can not only allow APS to take initiatives to stay within its competency boundary in untested situations, but also provide users/stakeholders with assurances that allow them to properly calibrate trust in (and hence make proper use of) intelligent APS \cite{Israelsen2017-ym}. 

These properties are especially important for APS that must rely heavily on non-deterministic algorithms for decision-making under uncertainty, i.e. to efficiently make approximate inferences with imperfect models, learn from limited data, and execute potentially risky actions with limited information. 
Whereas most relevant and recent work on algorithmic introspection and meta-reasoning to date has focused on outcome-based analyses for  AI/learning agents with narrow well-defined tasks, holistic process-based techniques for algorithmic competency boundary self-assessment are needed to accommodate broader classes of APS operating in complex, dynamic and uncertain real-world settings -- whose computational models and approximations are expected to break down in less obvious/foreknown ways. %\nisar{meh?}

This paper presents and builds on a recently developed algorithmic framework for computing and evaluating self-assessments in APS that leads to shorthand metrics of \emph{machine self-confidence}. Self-confidence is defined as an APS' perceived ability to achieve assigned goals after accounting for uncertainties in its knowledge of the world, its own state, and its own reasoning and execution abilities \cite{Aitken2016-cv, Aitken2016-fb, Sweet2016-tz}. 
Algorithmic computation of self-confidence is strongly linked to model-based assessments of probabilities pertaining to task outcomes and completion---but crucially goes further to provide insight into how well an APS's processes for decision-making, learning, perception, etc. are matched to intended tasks \cite{Hutchins2015-if}. 
We argue that the short-hand insight provided by self-confidence assessments can serve as a transparent and decomposable/traceable feedback signal to anticipate degraded, nominal, or enhanced APS performance, %adapt autonomous behavior, 
and thereby can be used to calibrate user trust in APS for uncertain task settings. 

The main contributions of this paper include: 1) A formal definition of `solver-quality' which is one of several factors that make up `self-confidence'. Herein, solver-quality is presented as a metric for assessing how competent an MDP solver is for a given task. 2) Solver-quality is then derived borrowing inspiration from empirical hardness models (EHMs \cite{Leyton-Brown2009-yr}. 3) Solver-quality is then evaluated using numerical experiments. The paper is organized as follows: In Section~\ref{sec:background} we further explore motivations and background for self-confidence, including concepts like trust between humans and autonomous systems, and a useful example application. In Section~\ref{sec:self-confidence} Factorized Machine Self-Confidence (\famsec) is introduced and a framework outlined. At the end of Section~\ref{sec:self-confidence} we turn our attention to one of the \famsec{} factors, `Solver Quality', and outline specific challenges and desiderata in the context of the broadly useful family of Markov Decision Process (MDP)-based planners. A learning-based technique for computing solver quality factors in MDP-family planners is then derived in Section~\ref{sec:methodology}. In Section~\ref{sec:results} we present results from numerical experiments for an unmanned autonomous vehicle navigation problem. Finally we present conclusions in Section~\ref{sec:conclusions}.

\section{Background and Related Work} \label{sec:background}
\brett{take the AHFE paper for a template in this section}
This section reviews several key concepts and related works which set the stage for our proposed computational machine self-confidence framework. To make the concepts discussed throughout the paper concrete and provide an accessible proof-of-concept testbed in later sections, we also describe a motivating APS application example inspired by ongoing research in unmanned robotic systems.  

\subsection{Autonomous Systems and User Trust}
\brett{Cut this to 1-2 medium paragraphs}
An APS is generally any physical agent comprised of a machine controlled by some form of software-based autonomy. Autonomy defines the ability of the system to perform a complex set of tasks with little/no human supervisory intervention for extended periods of time. This generally means that an APS has at least one or more of the capabilities of an artificially intelligent physical agent, i.e. reasoning, knowledge representation, planning, learning, perception, motion/manipulation, and/or communication \cite{Israelsen2017-ym}. 
Despite many popular myths and misconceptions, an APS always interacts with a human user in some way \cite{Bradshaw2013-ck}. 
That is, the aforementioned capabilities are the means by which an APS achieves some \emph{intended} degree of self-sufficiency and self-directedness for tasks that are \emph{delegated} by a user in order to meet an `intent frame' (desired set of goals, plans, constraints, stipulations, and/or value statements) \cite{Miller2014-av}. `Transparency' in this context thus shifts primary concern away from details of how exactly an APS accomplishes a task, towards knowing whether an autonomous system can/cannot execute the task per the user's intent frame. 
In cases where users must re-examine delegated tasks, the ability to interrogate an APS for details related to how tasks would be executed or why tasks can/cannot be completed become an important follow-on consideration for transparency (i.e. on a need to know `drill-down' basis). 
%%\nisar{hook to `drill down' req: user in high-consequence situation would want to know why/why not...and ask more questions as needed or as time/context permits... }

This view naturally sets up several questions related to user trust in autonomous systems. Trust defines a user's willingness and security in depending on an APS to carry out a delegated set of tasks, having taken into consideration its characteristics and capabilities. 
We focus here on the problem of how an APS can be designed to actively assist users in appropriately calibrating their trust in the APS. As surveyed in \cite{Israelsen2017-ym}, several broad classes of \emph{algorithmic assurances} for APS have been developed, where an assurance is defined as any property or behavior that can serve to increase or decrease a user's trust. 
Good assurances are challenging to develop because they must allow users to gain better insight and understanding of APS behaviors for effectively managing operations, without undermining autonomous operations or burdening users in the process. 
Many assurance strategies, such as value alignment \cite{Dragan2014-gu} (where an APS adapts its behavioral objectives with a user's intent frame via interactive learning) and interpretable reasoning \cite{Ruping2006-xj} (where algorithmic capabilities for planning, learning, reasoning, etc. are made accessible and easy to understand for non-expert users) put the onus on the APS (and designers) to integrate naturally transparent trust-calibrating behaviors into core system functionality. 
Other strategies, such as those based on post hoc explanation for learning and reasoning systems \cite{Lacave2004-gq, Ribeiro2016-uc} and data visualization \cite{Sacha2017-hf}, require users to render their own judgments via processing of information provided by the APS (possibly in response to specific user queries). 
Indeed, while the full range of assurance design strategies for APS have much in common with techniques for ensuring transparency and accountability for more general AI and learning systems, assurances based on self-monitoring offer an especially promising path for APS competency assessment. 

\subsection{Self-Monitoring and Self-Confidence}
\brett{Cut this to 1-2 medium paragraphs}
State of the art machine learning and statistical AI methods have ushered in major improvements to APS capabilities in recent years. 
Yet, as these methods and capabilities continue to improve and find new high-consequence applications, resulting APS implementations are also becoming more complex, opaque and difficult for users (as well as designers and certifying authorities) to fully comprehend. 
In particular, for sophisticated APS characterized by uncertainty-based reasoning and data-driven learning, it can become extremely difficult to make precise predictions about APS behavior and performance limits in noisy, untested, and `out of scope' task conditions with any degree of certainty. Formal verification and validation tools could be used to tackle these issues at design time, but do not provide assurances that can be readily conveyed to or understood by (non-expert) users at run-time. 
It can thus be argued that the task of assessing APS competency at run-time is in general so complex and burdensome that it should also be delegated to the APS itself. 

This leads to consideration of algorithmic self-monitoring methods, e.g. for introspective reasoning/learning \cite{Huang2017-lk}, fault diagnosis and computational meta-reasoning/meta-learning \cite{grant2018recasting}. 
While promising for a wide variety of applications, these methods depend heavily on task outcome and performance assessments, and often require data intensive evaluations. 
As such, these methods are often best-suited to APS with narrow, well-defined, capabilities and few computational resource constraints. 
However, many current and future APS must operate in open-ended task settings in physical environments with significant computational limitations (due to constrained platform size, weight, power, etc.). 
The interpretation of `favorable vs. unfavorable' task outcomes can also shift in subtle yet significant ways that may not be obvious to non-expert users, i.e. depending on the interactions of designed APS capabilities and task context (all of which may also change drastically over the course of a given operational instance). 

These limitations motivate consideration of \emph{process-based assessment} techniques that allow APS to more generally self-qualify their capabilities for a particular task by evaluating and reporting their associated degree of `self-trust' or \emph{self-confidence}. 
As evidenced by recent work in neurocomputational modeling of decision-making for visual-motor tasks,
self-confidence reporting in humans generally requires second-order reasoning about uncertainties associated with particular task outcomes, i.e. assessments of `uncertainties in uncertainty' as well as of one's own reasoning processes \cite{Adler2016-oi}.  This resonates with the machine self-confidence concept put forth by \cite{Hutchins2015-if}, who proposed using human expert evaluations of specific APS capabilities to manually encode where and when these may break down in particular tasking situations. 
Several formal definitions and techniques for allowing APS to automatically compute their own machine self-confidence scores in the context of different tasks and capability assessments have been proposed recently.
For instance, Kuter and Miller \cite{Kuter2015-qh} proposed to evaluate \emph{plan stability} for systems that rely on hierarchical task planning algorithms, using formal counter-planning methods to determine threatening contingencies for a given plan and plan repair techniques in order to assess the adaptability of that plan to circumvent those contingencies. 
This relies heavily on fixed knowledge bases and ontologies, and so only supports assessments for well-understood environments, tasks, systems, and contexts. 
These and other approaches are reviewed in \cite{Sweet2016-tz}, as well as in \cite{Israelsen2017-ym} in the context of algorithmic interactions for human-autonomous system trust relationships. %, where self-confidence is identified as an explicit assurance in a human-autonomy trust relationship. 
For the sake of brevity, we restrict attention to the definition of self-confidence used in this work: \textbf{An agent's perceived ability to achieve assigned goals (within a defined region of autonomous behavior) after accounting for (1) uncertainties in its knowledge of the world, (2) uncertainties of its own state, and (3) uncertainties about its reasoning process and execution abilities.}

\subsection{MDP-based Planning and Learning}
\brett{Cut this to 1-2 medium paragraphs}
The diversity of factors that influence APS self-confidence requires a rich modeling approach for algorithmic assessment. We will therefore establish algorithmic realizations of self-confidence assessments by initially studying APS capabilities that can be defined or modeled via Markov decision processes (MDPs). MDPs are composed of finite states and actions that partially resolve the nondeterminism in the state transitions by deciding from what probability distribution $p(\cdot)$ the next state will be sampled. The co-existence of nondeterministic and stochastic choices in MDPs are expressive enough to account for a range of uncertainties including adversarial environmental factors and inaccuracies in execution. %%, and limitations in prior knowledge (e.g. imperfect knowledge of $p(\cdot)$).  
Since MDPs also have well-established connections to other widely used approaches for autonomous decision-making and learning under uncertainty, such as partially observable MDPs (POMDPs) for decision-making in limited observability environments and reinforcement learning for decision-making with incomplete model information \cite{Kochenderfer2015-uu}, they provide an ideal starting point for an initial analysis of self-confidence that can be generalized in future work. 

More formally, we consider generic MDP formulations of a task \task{} delegated to an APS. In an MDP framing of \task{}, the autonomous agent must find an optimal policy $\pi = u(x)$ for an MDP with dynamical state $x$ and actions $u$, such that the objective function
$U = \mathbb{E} \left[\sum_{k=0}^{\infty} \gamma^i r(x_k,u_k) \right]$ is maximized for all times $k=0,...,\infty$ --  
where $R(x_k,u_k)$ rewards (penalizes) the APS for being in (un)desirable states and taking (un)desirable actions, $\mathbb{E}[\cdot]$ is the expected value over all possible future states, and $\gamma \in (0,1]$ is a (tunable) future discount factor. 
Given any $u_k$, the state $x_k$ updates via a Markov probabilistic transition model $x_{k+1} \sim p(x_{k+1}|x_{k},u_{k})$,  
i.e. $x_{i}$ is fully observed at time $i$ (no sensor noise), while transitions $i\rightarrow k+1$ have random perturbations.
In a fully posed MDP, $\pi$ is the optimal state-action policy, which can be recovered from Bellman's equation via dynamic programming. 
However, in many practical situations, policy approximations $\tilde{\pi}$ may still be required, e.g. to cope with very large state dimensions or structured uncertainties in the state transition distribution \cite{Kochenderfer2015-uu}. 
%\nisar{TODO: also help bridge gap to \famsec{} in next section...\famsec{} not exclusive to MDPs, but it's a sensible place to start...note: approximations of reality needed to set up models of decision processes, and then require even more approximations on top of these to actually implement...so this gives a good consequential focus to develop s/c framework}
    
	\begin{figure}[t]%[htbp]
    	\centering
     	\includegraphics[width=0.35\textwidth]{Figures/RoadNet}
    	\caption{UGV in road network evading pursuer with information from noisy UGS.} 
        \label{fig:RoadNet}
        \vspace{-0.2cm}
    \end{figure}

\subsubsection{Donut Delivery Application} \label{sec:donut_delivery}
Consider a concrete grounding example called the `Donut Delivery' problem (based on a `VIP escort' scenario~\cite{Humphrey2012-lr}). As shown in Fig. \label{fig:RoadNet} , an autonomous donut delivery truck (ADT) navigates a road network in order to reach a delivery destination while avoiding a motorcycle gang (MG) that will steal the donuts if they cross paths with the delivery truck. The motorcycle gang's location is unknown but can be estimated using intermittent updates from unmanned ground sensors (UGS). The delivery truck's decision space involves selecting a sequence of discrete actions (i.e. go straight, turn left, turn right, go back, stay in place). The ADT motion, UGS readings, and MG behavior are stochastic, and the problems of decision making and sensing are strongly coupled: some trajectories through the network might allow the ADT to localize the MG before heading to the delivery destination but incur a high time penalty); other trajectories may afford rapid delivery with high MG location uncertainty but increase the risk of getting caught by the MG, which can take multiple paths. A human supervisor monitors the ADT during operation. The supervisor does not have detailed knowledge or control of the ADT -- but can interrogate its actions, and (based on whatever limited amount of information is provided to the supervisor) can provide `go' or `no go' commands to proceed with or abort the delivery operation before it commences. 

The physical states describing the combined motion of the ADT (whose states are always perfectly observable) and MG can be discretized in time and space to produce a Markov process model defined by some initial joint state probability distribution and joint state transition matrix, which depends on the steering actions taken by the ADT. The probability of obtaining `detection' and `no detection' data from each UGS given the true state of the MG can be modeled and used to update probabilistic beliefs about the state of the chaser. Finally, a reward function $R(x_k,u_k) = R_k$ can be specified for each time step $k$ to encode user preferences over the combined state of the ADT and MG, e.g. $R_k = -200$ for each time step the ADT is not co-located with the MG but not yet at the exit, $R_k= -2000$ if the ADT and MG are co-located, and $R_k=+2000$ if the ADT reaches the exit without getting caught. Given these elements, the ADT's navigational planning and decision-making problem may be generally formulated as a POMDP. If the MG's state is fully observable at each step $k$ (e.g. due to very reliable and accurate UGS that cover all areas of the road network), the problem reduces to an MDP. 
