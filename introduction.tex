\section{Introduction}

\nisar{ACM Survey paper missing journal venue info at end: should read as `in press' with ACM CSUR...}

Autonomy defines a robot's (or other physical system's) ability to perform a complex tasks without human intervention for extended periods of time. This requires at least one or more functional capabilities of an artificially intelligent agent, i.e. reasoning, knowledge representation, planning, learning, perception, motion/manipulation, and communication \cite{Russell2010-wv}. 
However, since autonomous robots always interact with human users in some way \cite{Bradshaw2013-ck}, these functional abilities are only the means by which a robot achieves some \emph{intended} degree of self-sufficiency and self-directedness for tasks \emph{delegated} by a user to meet desired goals, plans, constraints, stipulations, or value statements \cite{Miller2014-av}. 
Nevertheless, as functional capabilities keep improving and finding new applications, autonomous robots remain at risk of becoming too complex and difficult for users (as well as designers and stakeholders) to fully comprehend.  
This state of affairs has drawn much attention to the issue of trust, i.e. a user's willingness to depend on a robot to carry out a delegated set of tasks, in light of their expectations, understanding, and perception of the robot's capabilities. 

This work examines how autonomous robots can use algorithmic self-assessment strategies to actively `calibrate' factors that inform user trust in delegated supervised task settings. 
As distinct from research on autonomous introspection for self-improvement \cite{NathanMichaelMichaelNathan, Posner}, 
we are not motivated here by whether/how a robot can adapt its functional capabilities to \emph{improve} its performance on a particular task -- but rather by the more basic question of whether a robot has a `reasonable' or contextually useful understanding of how its current functional capabilities would lead it to \emph{actually} perform on that task. As an analogy: consider how, say, a correctly tuned Kalman filter can be trusted to `know' its true state error statistics through the computed state error covariance matrix (i.e. such that the assumed approximate filtering distribution model can be statistically validated by truth model simulations and correctly fits actual sensor data) \cite{Bar-Shalom2001-tg}. We argue here that a robot with a correct sense of `self-trust' could also be similarly trusted to know its own performance limitations on a given task --- provided that the self-trust measure actually reflects how well the robot's underlying functionality (with all the attendant models, data, assumptions, algorithms, and other programmed/pre-ordained approximations of reality) fits the task. As such, we primarily seek to explore and establish meaningful self-trust measures. 

Our work can also be contrasted to the recent spate of work on explainable/interpretable learning and AI for robotics \cite{...ohgod,somany..}, where the aim primarily is to enable the robot to provide some tailored form of post hoc justification, summary, or notification of critical actions/states encountered, after/while the robot performs a task already delegated to it (often alongside a co-equal human partner). In our work, we want to understand whether and how the robot could assess \emph{on its own} if the task that could be delegated to it by a human supervisor (i.e. not a co-equal) falls within its competency boundaries, and thus whether the task to be delegated is even appropriate for or realistically achievable by that robot in the first place. %(thus potentially altogether avoiding the need for in situ/post hoc explanations of critical actions/states). 
%or research on explainable/interpretable learning and AI \cite{bunchofstuff}
Such considerations are vital in high-risk/high-cost applications, e.g. for autonomous unmanned delivery in urban environments; emergency response; defense and security; orbital structure assembly and planetary exploration, to name a few \nisar{...cite..}. \nisar{..in these and other settings: supervisor is ultimately responsible, and robot may not be able to `just fail and try again' ... at same time, supervisor has flexibility to modify tasks to fit within competency bounds, or only to delegate tasks that are suitable, if not critical despite the potential risks...}  

These considerations naturally lead us to algorithmic approaches for introspective meta-analysis of autonomy, i.e. analysis and assessment of the processes used to define and implement a robot's autonomous functional abilities. This builds on the concept of `machine self-confidence' put forth by \cite{Hutchins2015-if}, which proposed using human expert evaluations of specific autonomous system capabilities to manually encode where and when these may break down in particular tasking situations. However, as argued above: to be useful and scalable for real applications, autonomous robots should be able to form such assessments on their own. While several definitions and algorithmic approaches have been proposed recently which enable autonomous systems to automatically generate self-confidence scores in the context of different tasks and uncertainty-based capability assessments \cite{Sweet2016-tz, Israelsen2017-ym},  this work restricts attention to the following general definition of self-confidence: \textit{an agent's perceived ability to achieve assigned goals (within a defined region of autonomous behavior) after accounting for (1) uncertainties in its knowledge of the world, (2) uncertainties of its own state, and (3) uncertainties about its reasoning process and execution abilities.} %\nisar{can move prev sent to \famsec{} section?} 

\nisar{for me todo:...modify this parag to be the contributions of the paper...then do Section outline...}
However, a framework for computing and communicating self-confidence for general decision-making autonomy architectures has yet to be established. \nisar{...add in more detail per response to Dale...} Furthermore, it has not yet been confirmed experimentally whether (or in what contexts) human-APS interfaces that incorporate self-confidence reporting improve the ability of users to delegate of tasks within APS competency limits, compared to status quo interfaces that do not use such reporting. 
This paper provides results aimed at addressing both of these gaps. \nisar{todo later: provide crisp direct statement of contributions made by this paper here...move from last paragraph within sections outline to here...}
%\nisar{can probably trim this parag down a bit, after editing per comment at end...}
\nisar{todo later: edit accordingly...}
In Section 2, we motivate the machine self-confidence idea and ground the concept for autonomous planning problems in the context of general Markov Decision Processes (MDPs), using a concrete uncertain navigation application example where tasks are delegated by a user. We review a factorization based framework for self-confidence assessment (\famsec), and in Section 3 present a novel strategy for computing the so-called `solver quality' factor of self-confidence, drawing inspiration from empirical hardness modeling literature \cite{Leyton-Brown2009-yr}. Numerical examples for the UGV navigation problem under different MDP solver, parameter, and environment conditions, indicate that the self-confidence scores exhibit desired properties. Section 4 describes the setup and initial outcomes of experimental trials to investigate the effects of reporting self-confidence to users on simulated instances of the UGV navigation problem. The results show significantly improved delegated task performance outcomes in conditions where self-confidence feedback is provided to users vs. conditions where no self-confidence feedback is provided, providing favorable evidence for the efficacy of self-confidence reporting. Section 5 presents conclusions and avenues for future investigation. %\nisar{only mention solver quality -- what about outcome assessment? this is also in the data...}
